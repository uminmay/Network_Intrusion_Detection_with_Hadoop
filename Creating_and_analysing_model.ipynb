{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef80b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41b274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory','4g')\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances',20)\n",
    "sc = SparkContext('yarn', 'kdd99', conf=conf)\n",
    "hc=HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315fbc2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc.sql(\"USE itv000684_kdd99data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e04fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdd = hc.table(\"kdd99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53602a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = kdd.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e34b6846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>protocol_type</th><th>service</th><th>flag</th><th>is_anomaly</th><th>duration</th><th>src_bytes</th><th>dst_bytes</th><th>wrong_fragment</th><th>urgent</th><th>hot</th><th>num_failed_logins</th><th>num_compromised</th><th>root_shell</th><th>su_attempted</th><th>num_root</th><th>num_file_creations</th><th>num_shells</th><th>num_access_files</th><th>num_outbound_cmds</th><th>count</th><th>srv_count</th><th>serror_rate</th><th>srv_serror_rate</th><th>rerror_rate</th><th>srv_rerror_rate</th><th>same_srv_rate</th><th>diff_srv_rate</th><th>srv_diff_host_rate</th><th>dst_host_count</th><th>dst_host_srv_count</th><th>dst_host_same_srv_rate</th><th>dst_host_diff_srv_rate</th><th>dst_host_same_src_port_rate</th><th>dst_host_srv_diff_host_rate</th><th>dst_host_serror_rate</th><th>dst_host_srv_serror_rate</th><th>dst_host_rerror_rate</th><th>dst_host_srv_rerror_rate</th></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.67</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>15.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.53</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>29.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.52</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>43.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>53.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>97.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.51</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>127.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>131.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>233.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>icmp</td><td>eco_i</td><td>SF</td><td>ipsweep.</td><td>0</td><td>8</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>255.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>0.5</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------+-------+----+----------+--------+---------+---------+--------------+------+---+-----------------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
       "|protocol_type|service|flag|is_anomaly|duration|src_bytes|dst_bytes|wrong_fragment|urgent|hot|num_failed_logins|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|\n",
       "+-------------+-------+----+----------+--------+---------+---------+--------------+------+---+-----------------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|               3.0|                   1.0|                   0.0|                        1.0|                       0.67|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|              15.0|                   1.0|                   0.0|                        1.0|                       0.53|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|              29.0|                   1.0|                   0.0|                        1.0|                       0.52|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|              43.0|                   1.0|                   0.0|                        1.0|                       0.51|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|              53.0|                   1.0|                   0.0|                        1.0|                       0.51|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|              97.0|                   1.0|                   0.0|                        1.0|                       0.51|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             127.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             131.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             233.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "|         icmp|  eco_i|  SF|  ipsweep.|       0|        8|        0|             0|     0|  0|                0|              0|         0|           0|       0|                 0|         0|               0|                0|    1|        1|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                        0.5|                 0.0|                     0.0|                 0.0|                     0.0|\n",
       "+-------------+-------+----+----------+--------+---------+---------+--------------+------+---+-----------------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860aff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "services = train_data.withColumnRenamed('service','srvc').select('srvc').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b2819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter and remove any rows with a service not trained upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f1de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.join(services, test_data.service == services.srvc)\n",
    "#test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063bf404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set has 3429322 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"training set has \" + str(train_data.count()) + \" instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c023b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set has 1469108 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"test set has \" + str(test_data.count()) + \" instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35621cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffbd1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = StringIndexer(inputCol=\"protocol_type\", outputCol=\"protocol-cat\")\n",
    "index2 = StringIndexer(inputCol=\"service\", outputCol=\"service-cat\")\n",
    "index3 = StringIndexer(inputCol=\"flag\", outputCol=\"flag-cat\")\n",
    "index4 = StringIndexer(inputCol=\"is_anomaly\", outputCol=\"label\")\n",
    "onehotencode = OneHotEncoder(inputCol=\"service-cat\", outputCol=\"service-onehotencode\")\n",
    "\n",
    "feat_columns = [col for col in kdd.columns +\n",
    "               ['protocol-cat','service-onehotencode','flag-cat','label']\n",
    "               if col not in ['protocol_type','service','flag','is_anomaly']]\n",
    "vectorAssembler = VectorAssembler(inputCols = feat_columns, outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1e2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomjungle = RandomForestClassifier(numTrees=500, maxDepth=6, maxBins=80,seed=42)\n",
    "pipeline = Pipeline(stages=[index1,index2,index3,index4,onehotencode, vectorAssembler, randomjungle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f97e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "themodel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b78ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "themodel.save('/user/itv000684/kdd/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c0b53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model = PipelineModel.load('/user/itv000684/kdd/model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "786379d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(test_data).select(\"label\",\"prediction\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "836155f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def eval_metrics(lap):\n",
    "    labels = lap.select(\"label\").distinct().toPandas()['label'].tolist()\n",
    "    tpos = [lap.filter(lap.label == x).filter(lap.prediction == x).count() for x in labels]\n",
    "    fpos = [lap.filter(lap.label == x).filter(lap.prediction != x).count() for x in labels]\n",
    "    fneg = [lap.filter(lap.label != x).filter(lap.prediction != x).count() for x in labels]\n",
    "    precision = zip(labels,[float(tp)/(tp+fp+1e-50) for (tp,fp) in zip(tpos,fpos)])\n",
    "    recall = zip(labels, [float(tp)/(tp+fn+1e-50) for (tp,fn) in zip(tpos,fneg)])\n",
    "    return(precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5afca53",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1656.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, true, [id=#7735]\n+- *(3) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#17831L])\n   +- *(3) Project\n      +- *(3) BroadcastHashJoin [service#3], [srvc#1752], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#7721]\n         :  +- *(1) Project [service#3]\n         :     +- *(1) Filter ((isnotnull(is_anomaly#5) AND (is_anomaly#5 = teardrop.)) AND isnotnull(service#3))\n         :        +- *(1) Sample 0.7, 1.0, false, 42\n         :           +- *(1) Project [service#3, is_anomaly#5]\n         :              +- *(1) Sort [protocol_type#2 ASC NULLS FIRST, service#3 ASC NULLS FIRST, flag#4 ASC NULLS FIRST, is_anomaly#5 ASC NULLS FIRST, duration#6 ASC NULLS FIRST, src_bytes#7 ASC NULLS FIRST, dst_bytes#8 ASC NULLS FIRST, wrong_fragment#9 ASC NULLS FIRST, urgent#10 ASC NULLS FIRST, hot#11 ASC NULLS FIRST, num_failed_logins#12 ASC NULLS FIRST, num_compromised#13 ASC NULLS FIRST, root_shell#14 ASC NULLS FIRST, su_attempted#15 ASC NULLS FIRST, num_root#16 ASC NULLS FIRST, num_file_creations#17 ASC NULLS FIRST, num_shells#18 ASC NULLS FIRST, num_access_files#19 ASC NULLS FIRST, num_outbound_cmds#20 ASC NULLS FIRST, count#21 ASC NULLS FIRST, srv_count#22 ASC NULLS FIRST, serror_rate#23 ASC NULLS FIRST, srv_serror_rate#24 ASC NULLS FIRST, rerror_rate#25 ASC NULLS FIRST, ... 14 more fields], false, 0\n         :                 +- *(1) ColumnarToRow\n         :                    +- FileScan orc itv000684_kdd99data.kdd99[protocol_type#2,service#3,flag#4,is_anomaly#5,duration#6,src_bytes#7,dst_bytes#8,wrong_fragment#9,urgent#10,hot#11,num_failed_logins#12,num_compromised#13,root_shell#14,su_attempted#15,num_root#16,num_file_creations#17,num_shells#18,num_access_files#19,num_outbound_cmds#20,count#21,srv_count#22,serror_rate#23,srv_serror_rate#24,rerror_rate#25,... 14 more fields] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv000684/warehouse/itv000684_kdd99data.db/k..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<protocol_type:string,service:string,flag:string,is_anomaly:string,duration:int,src_bytes:i...\n         +- *(3) HashAggregate(keys=[srvc#1752], functions=[], output=[srvc#1752])\n            +- Exchange hashpartitioning(srvc#1752, 200), true, [id=#7728]\n               +- *(2) HashAggregate(keys=[srvc#1752], functions=[], output=[srvc#1752])\n                  +- *(2) Project [service#3 AS srvc#1752]\n                     +- *(2) Filter isnotnull(service#3)\n                        +- InMemoryTableScan [service#3], [isnotnull(service#3)]\n                              +- InMemoryRelation [protocol_type#2, service#3, flag#4, is_anomaly#5, duration#6, src_bytes#7, dst_bytes#8, wrong_fragment#9, urgent#10, hot#11, num_failed_logins#12, num_compromised#13, root_shell#14, su_attempted#15, num_root#16, num_file_creations#17, num_shells#18, num_access_files#19, num_outbound_cmds#20, count#21, srv_count#22, serror_rate#23, srv_serror_rate#24, rerror_rate#25, ... 14 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                    +- *(1) Sample 0.0, 0.7, false, 42\n                                       +- *(1) Sort [protocol_type#2 ASC NULLS FIRST, service#3 ASC NULLS FIRST, flag#4 ASC NULLS FIRST, is_anomaly#5 ASC NULLS FIRST, duration#6 ASC NULLS FIRST, src_bytes#7 ASC NULLS FIRST, dst_bytes#8 ASC NULLS FIRST, wrong_fragment#9 ASC NULLS FIRST, urgent#10 ASC NULLS FIRST, hot#11 ASC NULLS FIRST, num_failed_logins#12 ASC NULLS FIRST, num_compromised#13 ASC NULLS FIRST, root_shell#14 ASC NULLS FIRST, su_attempted#15 ASC NULLS FIRST, num_root#16 ASC NULLS FIRST, num_file_creations#17 ASC NULLS FIRST, num_shells#18 ASC NULLS FIRST, num_access_files#19 ASC NULLS FIRST, num_outbound_cmds#20 ASC NULLS FIRST, count#21 ASC NULLS FIRST, srv_count#22 ASC NULLS FIRST, serror_rate#23 ASC NULLS FIRST, srv_serror_rate#24 ASC NULLS FIRST, rerror_rate#25 ASC NULLS FIRST, ... 14 more fields], false, 0\n                                          +- *(1) ColumnarToRow\n                                             +- FileScan orc itv000684_kdd99data.kdd99[protocol_type#2,service#3,flag#4,is_anomaly#5,duration#6,src_bytes#7,dst_bytes#8,wrong_fragment#9,urgent#10,hot#11,num_failed_logins#12,num_compromised#13,root_shell#14,su_attempted#15,num_root#16,num_file_creations#17,num_shells#18,num_access_files#19,num_outbound_cmds#20,count#21,srv_count#22,serror_rate#23,srv_serror_rate#24,rerror_rate#25,... 14 more fields] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv000684/warehouse/itv000684_kdd99data.db/k..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<protocol_type:string,service:string,flag:string,is_anomaly:string,duration:int,src_bytes:i...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:382)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2981)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2980)\n\tat sun.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job 257 cancelled because SparkContext was shut down\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:188)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:116)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:210)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:100)\n\tat org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:626)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:762)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:169)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:95)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:243)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:167)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\nCaused by: org.apache.spark.SparkException: Job 257 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:122)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:392)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:182)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9929cbab828c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mordered_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_anomaly\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-9929cbab828c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mordered_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_anomaly\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1656.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, true, [id=#7735]\n+- *(3) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#17831L])\n   +- *(3) Project\n      +- *(3) BroadcastHashJoin [service#3], [srvc#1752], Inner, BuildLeft\n         :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#7721]\n         :  +- *(1) Project [service#3]\n         :     +- *(1) Filter ((isnotnull(is_anomaly#5) AND (is_anomaly#5 = teardrop.)) AND isnotnull(service#3))\n         :        +- *(1) Sample 0.7, 1.0, false, 42\n         :           +- *(1) Project [service#3, is_anomaly#5]\n         :              +- *(1) Sort [protocol_type#2 ASC NULLS FIRST, service#3 ASC NULLS FIRST, flag#4 ASC NULLS FIRST, is_anomaly#5 ASC NULLS FIRST, duration#6 ASC NULLS FIRST, src_bytes#7 ASC NULLS FIRST, dst_bytes#8 ASC NULLS FIRST, wrong_fragment#9 ASC NULLS FIRST, urgent#10 ASC NULLS FIRST, hot#11 ASC NULLS FIRST, num_failed_logins#12 ASC NULLS FIRST, num_compromised#13 ASC NULLS FIRST, root_shell#14 ASC NULLS FIRST, su_attempted#15 ASC NULLS FIRST, num_root#16 ASC NULLS FIRST, num_file_creations#17 ASC NULLS FIRST, num_shells#18 ASC NULLS FIRST, num_access_files#19 ASC NULLS FIRST, num_outbound_cmds#20 ASC NULLS FIRST, count#21 ASC NULLS FIRST, srv_count#22 ASC NULLS FIRST, serror_rate#23 ASC NULLS FIRST, srv_serror_rate#24 ASC NULLS FIRST, rerror_rate#25 ASC NULLS FIRST, ... 14 more fields], false, 0\n         :                 +- *(1) ColumnarToRow\n         :                    +- FileScan orc itv000684_kdd99data.kdd99[protocol_type#2,service#3,flag#4,is_anomaly#5,duration#6,src_bytes#7,dst_bytes#8,wrong_fragment#9,urgent#10,hot#11,num_failed_logins#12,num_compromised#13,root_shell#14,su_attempted#15,num_root#16,num_file_creations#17,num_shells#18,num_access_files#19,num_outbound_cmds#20,count#21,srv_count#22,serror_rate#23,srv_serror_rate#24,rerror_rate#25,... 14 more fields] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv000684/warehouse/itv000684_kdd99data.db/k..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<protocol_type:string,service:string,flag:string,is_anomaly:string,duration:int,src_bytes:i...\n         +- *(3) HashAggregate(keys=[srvc#1752], functions=[], output=[srvc#1752])\n            +- Exchange hashpartitioning(srvc#1752, 200), true, [id=#7728]\n               +- *(2) HashAggregate(keys=[srvc#1752], functions=[], output=[srvc#1752])\n                  +- *(2) Project [service#3 AS srvc#1752]\n                     +- *(2) Filter isnotnull(service#3)\n                        +- InMemoryTableScan [service#3], [isnotnull(service#3)]\n                              +- InMemoryRelation [protocol_type#2, service#3, flag#4, is_anomaly#5, duration#6, src_bytes#7, dst_bytes#8, wrong_fragment#9, urgent#10, hot#11, num_failed_logins#12, num_compromised#13, root_shell#14, su_attempted#15, num_root#16, num_file_creations#17, num_shells#18, num_access_files#19, num_outbound_cmds#20, count#21, srv_count#22, serror_rate#23, srv_serror_rate#24, rerror_rate#25, ... 14 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                    +- *(1) Sample 0.0, 0.7, false, 42\n                                       +- *(1) Sort [protocol_type#2 ASC NULLS FIRST, service#3 ASC NULLS FIRST, flag#4 ASC NULLS FIRST, is_anomaly#5 ASC NULLS FIRST, duration#6 ASC NULLS FIRST, src_bytes#7 ASC NULLS FIRST, dst_bytes#8 ASC NULLS FIRST, wrong_fragment#9 ASC NULLS FIRST, urgent#10 ASC NULLS FIRST, hot#11 ASC NULLS FIRST, num_failed_logins#12 ASC NULLS FIRST, num_compromised#13 ASC NULLS FIRST, root_shell#14 ASC NULLS FIRST, su_attempted#15 ASC NULLS FIRST, num_root#16 ASC NULLS FIRST, num_file_creations#17 ASC NULLS FIRST, num_shells#18 ASC NULLS FIRST, num_access_files#19 ASC NULLS FIRST, num_outbound_cmds#20 ASC NULLS FIRST, count#21 ASC NULLS FIRST, srv_count#22 ASC NULLS FIRST, serror_rate#23 ASC NULLS FIRST, srv_serror_rate#24 ASC NULLS FIRST, rerror_rate#25 ASC NULLS FIRST, ... 14 more fields], false, 0\n                                          +- *(1) ColumnarToRow\n                                             +- FileScan orc itv000684_kdd99data.kdd99[protocol_type#2,service#3,flag#4,is_anomaly#5,duration#6,src_bytes#7,dst_bytes#8,wrong_fragment#9,urgent#10,hot#11,num_failed_logins#12,num_compromised#13,root_shell#14,su_attempted#15,num_root#16,num_file_creations#17,num_shells#18,num_access_files#19,num_outbound_cmds#20,count#21,srv_count#22,serror_rate#23,srv_serror_rate#24,rerror_rate#25,... 14 more fields] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv000684/warehouse/itv000684_kdd99data.db/k..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<protocol_type:string,service:string,flag:string,is_anomaly:string,duration:int,src_bytes:i...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:382)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2981)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2980)\n\tat sun.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job 257 cancelled because SparkContext was shut down\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:195)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:188)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:116)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:210)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:100)\n\tat org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:626)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:762)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:169)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:95)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:243)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:167)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:48)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:632)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:692)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 39 more\nCaused by: org.apache.spark.SparkException: Job 257 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:122)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:392)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:182)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "(precision, recall) = eval_metrics(results)\n",
    "ordered_labels = model.stages[3]._call_java(\"labels\")\n",
    "df = pd.DataFrame([(x, test_data.filter(test_data.is_anomaly == x).count(),y[1],z[1]) for x,y,z in zip(ordered_labels, sorted(precision, key = lambda x: x[0]), sorted(recall, key=lambda x: x[0]))], columns = ['type','count','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac18b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
